{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Body Problem: Neural Network Training\n",
    "\n",
    "A simple neural network is trained to learn approximate solutions to the three body problem.<br>\n",
    "Data is sampled from three body systems with parameters similar to planets in the solar system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import tensorflow as tf\n",
    "import rebound\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aliases\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from utils import load_vartbl, save_vartbl, plot_style, range_inc\n",
    "from tf_utils import gpu_grow_memory, TimeHistory\n",
    "from tf_utils import plot_loss_hist, EpochLoss, TimeHistory\n",
    "from tf_utils import Identity\n",
    "\n",
    "from orbital_element import OrbitalElementToConfig, ConfigToOrbitalElement, MeanToTrueAnomaly, G_\n",
    "from orbital_element import make_model_elt_to_cfg, make_model_cfg_to_elt\n",
    "\n",
    "from jacobi import CartesianToJacobi, JacobiToCartesian\n",
    "\n",
    "from g3b_data import make_traj_g3b, make_data_g3b, make_datasets_g3b, traj_to_batch\n",
    "from g3b_data import make_datasets_solar, make_datasets_hard\n",
    "from g3b_data import combine_datasets_g3b, combine_datasets_solar\n",
    "from sej_data import load_data_sej, make_datasets_sej, combine_datasets_sej\n",
    "\n",
    "from g3b_plot import plot_orbit_q, plot_orbit_v, plot_orbit_a, plot_orbit_energy, plot_orbit_element\n",
    "from g3b import KineticEnergy_G3B, PotentialEnergy_G3B, Momentum_G3B, AngularMomentum_G3B\n",
    "from g3b import VectorError, EnergyError\n",
    "from g3b import Motion_G3B, make_physics_model_g3b\n",
    "from g3b import fit_model\n",
    "from g3b_model_math import make_position_model_g3b_math, make_model_g3b_math\n",
    "from g3b_model_nn import make_position_model_g3b_nn, make_model_g3b_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set active GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[1:2], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grow GPU memory (must be first operation in TF)\n",
    "gpu_grow_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight serialization\n",
    "fname = '../data/g3b/g3b_train.pickle'\n",
    "vartbl = load_vartbl(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data for General Three Body Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of datasets to be loaded\n",
    "n_years = 100\n",
    "sample_freq = 10\n",
    "traj_size = n_years * sample_freq + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading data sets\n",
    "# num_data_sets = 50\n",
    "num_data_sets = 5\n",
    "batch_size = 64\n",
    "# num_gpus = 1\n",
    "# full_batch_size = num_gpus * batch_size\n",
    "\n",
    "# Set size of tiny data sets\n",
    "n_traj_tiny = batch_size\n",
    "\n",
    "# Set starting random seed\n",
    "seed0 = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/g3b/304606075.pickle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 10:19:59.034000 139762367096640 deprecation.py:323] From /home/michael/anaconda3/envs/nbody/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Create a tiny data set with one batch of solar type orbits\n",
    "ds_tiny_solar_trn, ds_tiny_solar_val, ds_tiny_solar_tst = \\\n",
    "    make_datasets_solar(n_traj=n_traj_tiny, vt_split=1.0, \n",
    "                        n_years=n_years, sample_freq=sample_freq,\n",
    "                        batch_size=batch_size, seed=seed0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined solar data sets\n",
    "# ds_solar_trn, ds_solar_val, ds_solar_tst = \\\n",
    "#     combine_datasets_solar(num_data_sets=num_data_sets, batch_size=batch_size, seed0=seed0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data for Perturbed Sun-Earth-Jupiter System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orbital perturbation scales\n",
    "sd_log_a = 0.01\n",
    "sd_log_e = 0.10\n",
    "sd_log_inc = 0.10\n",
    "sd_Omega = np.pi * 0.02\n",
    "sd_omega = np.pi * 0.02\n",
    "sd_f = np.pi * 0.02\n",
    "\n",
    "# Wrap into dictionary\n",
    "sej_sigma = {\n",
    "    'sd_log_a': sd_log_a,\n",
    "    'sd_log_e': sd_log_e,\n",
    "    'sd_log_inc': sd_log_inc,\n",
    "    'sd_Omega': sd_Omega,\n",
    "    'sd_omega': sd_omega,\n",
    "    'sd_f': sd_f\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/sej/1181713387.pickle.\n"
     ]
    }
   ],
   "source": [
    "# Create a tiny data set with one batch of perturbed SEJ orbits\n",
    "ds_tiny_sej_trn, ds_tiny_sej_val, ds_tiny_sej_tst = \\\n",
    "    make_datasets_sej(n_traj=n_traj_tiny, vt_split=1.0, n_years=n_years, sample_freq=sample_freq,\n",
    "                      **sej_sigma,\n",
    "                      batch_size=batch_size, seed=seed0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/sej/2974512314.pickle.\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary for unperturbed orbits\n",
    "sej_sigma0 = {k: v*0.0 for k, v in sej_sigma.items()}\n",
    "\n",
    "# Create a tiny data set with the unperturbed SEJ system\n",
    "ds_sej0, _, _ = \\\n",
    "    make_datasets_sej(n_traj=n_traj_tiny, vt_split=0.0, n_years=n_years, sample_freq=sample_freq,\n",
    "                      **sej_sigma0,\n",
    "                      batch_size=batch_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3283064e-10, 0.0000000e+00, 2.9103830e-11],\n",
       "       [5.9604645e-08, 4.7683716e-07, 2.9103830e-11],\n",
       "       [4.7683716e-07, 9.5367432e-07, 2.9802322e-08]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that starting positions identical to roundoff\n",
    "inputs, outputs = list(ds_sej0.take(1))[0]\n",
    "np.std(inputs['q0'], axis=(0,))\n",
    "# inputs['q0'][1]-inputs['q0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999339e-07"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that calculated positions essentially identical\n",
    "np.sqrt(np.mean(np.square(np.std(outputs['q'], axis=(0,)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(outputs['q'][19]-outputs['q'][17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined SEJ data sets\n",
    "# ds_sej_trn, ds_sej_val, ds_sej_tst = \\\n",
    "#     combine_datasets_solar(num_data_sets=num_data_sets, batch_size=batch_size, seed0=seed0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alias ds_trn, ds_val, ds_tst to selected source\n",
    "\n",
    "# The selected data type for this analysis\n",
    "data_type = 'solar'\n",
    "\n",
    "# Tables mapping data type to tuple of data sets\n",
    "data_tiny_by_type = {\n",
    "    'solar': (ds_tiny_solar_trn, ds_tiny_solar_val, ds_tiny_solar_tst),\n",
    "    'SEJ': (ds_tiny_sej_trn, ds_tiny_sej_val, ds_tiny_sej_tst)\n",
    "}\n",
    "\n",
    "data_by_type = {\n",
    "    # 'solar': (ds_solar_trn, ds_solar_val, ds_solar_tst),\n",
    "    #'SEJ': (ds_sej_trn, ds_sej_val, ds_sej_tst)\n",
    "}\n",
    "\n",
    "# Perform the aliasing\n",
    "ds_tiny_trn, ds_tiny_val, ds_tiny_tst = data_tiny_by_type[data_type]\n",
    "# ds_trn, ds_val, ds_tst = data_by_type[data_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Math Model as a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_math = make_model_g3b_math(traj_size=traj_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0)\n",
    "\n",
    "loss = {'q': VectorError(name='q_loss'),\n",
    "        'v': VectorError(name='v_loss'),\n",
    "        'a': VectorError(regularizer=1.0, name='a_loss'),\n",
    "        'q0_rec': VectorError(name='q0_loss'),\n",
    "        'v0_rec': VectorError(name='v0_loss'),\n",
    "        'H': EnergyError(name='H_loss'),\n",
    "        'P': VectorError(name='P_loss', regularizer=1.0),\n",
    "        'L': VectorError(name='L_loss'),\n",
    "       }\n",
    "\n",
    "metrics = None\n",
    "\n",
    "loss_weights = {'q': 1.0,\n",
    "                'v': 1.0,\n",
    "                'a': 1.0,\n",
    "                'q0_rec': 1.0E4,\n",
    "                'v0_rec': 1.0E4,\n",
    "                'H': 1.0,\n",
    "                'P': 1.0,\n",
    "                'L': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the full mathematical model\n",
    "model_math.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({t: (64, 1001), q0: (64, 3, 3), v0: (64, 3, 3), m: (64, 3)}, {q: (64, 1001, 3, 3), v: (64, 1001, 3, 3), a: (64, 1001, 3, 3), orb_a: (64, 1001, 2), orb_e: (64, 1001, 2), orb_inc: (64, 1001, 2), orb_Omega: (64, 1001, 2), orb_omega: (64, 1001, 2), orb_f: (64, 1001, 2), q0_rec: (64, 3, 3), v0_rec: (64, 3, 3), T: (64, 1001), U: (64, 1001), H: (64, 1001), P: (64, 1001, 3), L: (64, 1001, 3)}), types: ({t: tf.float32, q0: tf.float32, v0: tf.float32, m: tf.float32}, {q: tf.float32, v: tf.float32, a: tf.float32, orb_a: tf.float32, orb_e: tf.float32, orb_inc: tf.float32, orb_Omega: tf.float32, orb_omega: tf.float32, orb_f: tf.float32, q0_rec: tf.float32, v0_rec: tf.float32, T: tf.float32, U: tf.float32, H: tf.float32, P: tf.float32, L: tf.float32})>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tiny_solar_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({t: (64, 1001), q0: (64, 3, 3), v0: (64, 3, 3), m: (64, 3)}, {q: (64, 1001, 3, 3), v: (64, 1001, 3, 3), a: (64, 1001, 3, 3), orb_a: (64, 1001, 2), orb_e: (64, 1001, 2), orb_inc: (64, 1001, 2), orb_Omega: (64, 1001, 2), orb_omega: (64, 1001, 2), orb_f: (64, 1001, 2), q0_rec: (64, 3, 3), v0_rec: (64, 3, 3), T: (64, 1001), U: (64, 1001), H: (64, 1001), P: (64, 1001, 3), L: (64, 1001, 3)}), types: ({t: tf.float32, q0: tf.float32, v0: tf.float32, m: tf.float32}, {q: tf.float32, v: tf.float32, a: tf.float32, orb_a: tf.float32, orb_e: tf.float32, orb_inc: tf.float32, orb_Omega: tf.float32, orb_omega: tf.float32, orb_f: tf.float32, q0_rec: tf.float32, v0_rec: tf.float32, T: tf.float32, U: tf.float32, H: tf.float32, P: tf.float32, L: tf.float32})>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tiny_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0161 - q_loss: 0.0094 - v_loss: 0.0049 - a_loss: 0.0017 - q0_rec_loss: 3.4913e-14 - v0_rec_loss: 2.9723e-14 - H_loss: 3.4942e-09 - P_loss: 6.2723e-21 - L_loss: 2.0751e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.016079716384410858,\n",
       " 0.009423083,\n",
       " 0.004922431,\n",
       " 0.0017341995,\n",
       " 3.4913343e-14,\n",
       " 2.9722542e-14,\n",
       " 3.4942307e-09,\n",
       " 6.2723486e-21,\n",
       " 2.075063e-14]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_math.evaluate(ds_tiny_trn)\n",
    "model_math.evaluate(ds_tiny_solar_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_math.evaluate(ds_tiny_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for neural network model architecture\n",
    "# hidden_sizes = [64, 16]\n",
    "hidden_sizes = []\n",
    "skip_layers = True\n",
    "traj_size = 1001\n",
    "\n",
    "# Training configuration\n",
    "reg = 1.0E2\n",
    "kernel_reg = reg\n",
    "activity_reg = reg\n",
    "learning_rate = 1.0E-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network model\n",
    "model_nn = make_model_g3b_nn(hidden_sizes=hidden_sizes, skip_layers=skip_layers, \n",
    "                             kernel_reg=kernel_reg, activity_reg=activity_reg,\n",
    "                             traj_size=traj_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# optimizer = keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "# optimizer = keras.optimizers.Adadelta()\n",
    "\n",
    "loss = {'q': VectorError(name='q_loss'),\n",
    "        'v': VectorError(name='v_loss'),\n",
    "        'a': VectorError(regularizer=1.0E-2, name='a_loss'),\n",
    "        'q0_rec': VectorError(name='q0_loss'),\n",
    "        'v0_rec': VectorError(name='v0_loss'),\n",
    "        'H': EnergyError(name='H_loss'),\n",
    "        'P': VectorError(name='P_loss', regularizer=1.0E-6),\n",
    "        'L': VectorError(name='L_loss'),\n",
    "       }\n",
    "\n",
    "metrics = None\n",
    "\n",
    "loss_weights = {'q': 1.0,\n",
    "                'v': 1.0,\n",
    "                'a': 1.0,\n",
    "                'q0_rec': 1.0E4,\n",
    "                'v0_rec': 1.0E4,\n",
    "                'H': 1.0,\n",
    "                'P': 1.0,\n",
    "                'L': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the NN model\n",
    "model_nn.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the NN model on the validation data\n",
    "model_nn.evaluate(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this to math model - should be the same before training\n",
    "# model_math.evaluate(ds_val)\n",
    "\n",
    "# Hard coded baseline losses\n",
    "loss_baseline_list = \\\n",
    "[0.01564649583971394,\n",
    " 0.0051587406,\n",
    " 0.0053154626,\n",
    " 0.005172265,\n",
    " 3.3106947e-14,\n",
    " 3.2552384e-14,\n",
    " 2.5221508e-08,\n",
    " 1.1691268e-14,\n",
    " 2.1020908e-14]\n",
    "\n",
    "# Baseline position loss\n",
    "q_loss_baseline = loss_baseline_list[1]\n",
    "\n",
    "# Table of baseline losses\n",
    "keys = ['loss', 'q_loss', 'v_loss', 'a_loss', 'q0_rec_loss', 'v0_rec_loss', 'H_loss', 'P_loss', 'L_loss']\n",
    "loss_baseline = {key: loss_baseline_list[i] for i, key in enumerate(keys)}\n",
    "# Set dummy batch_num and time\n",
    "loss_baseline['batch_num'] = 0\n",
    "loss_baseline['time'] = 0.0\n",
    "\n",
    "# Initialize history before training\n",
    "hist0 = {key: np.array([val], dtype=np.float32) for key, val in loss_baseline.items()}\n",
    "\n",
    "# Review baseline loss table\n",
    "# loss_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "suffix = '_'.join(str(sz) for sz in hidden_sizes)\n",
    "model_name = f'model_g3b_nn_{suffix}'\n",
    "model_h5 = f'../models/g3b/{model_name}.h5'\n",
    "hist_name = model_name.replace('model_', 'hist_')\n",
    "epochs = 1\n",
    "save_freq = 'epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to load model or train a single epoch\n",
    "try:\n",
    "    model_nn.load_weights(model_h5)\n",
    "    model_nn.compile(loss=loss, optimizer=optimizer, metrics=metrics, loss_weights=loss_weights)\n",
    "    hist = vartbl[hist_name]\n",
    "    print(f'Loaded {model_name} from {model_h5}.')\n",
    "except:\n",
    "    print(f'Unable to load {model_name} from {model_h5}. Fitting...')\n",
    "    hist = fit_model(model=model_nn, \n",
    "                     ds=ds_trn, \n",
    "                     epochs=epochs,\n",
    "                     save_freq=save_freq,\n",
    "                     prev_history = hist0, \n",
    "                     batch_num=1)\n",
    "    vartbl[hist_name] = hist\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 50\n",
    "num_epochs = 1\n",
    "for i in range_inc(1, num_epochs):\n",
    "    ts = datetime.datetime.now()\n",
    "    st = ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f'*** Training loop {i:3} *** - {st}')\n",
    "    hist = fit_model(model=model_nn, \n",
    "                     ds=ds_trn, \n",
    "                     epochs=epochs,\n",
    "                     loss=loss, \n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics,\n",
    "                     save_freq=save_freq,\n",
    "                     prev_history = hist, \n",
    "                     batch_num=i)\n",
    "    vartbl[hist_name] = hist\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "fig, ax = plot_loss_hist(hist=hist, model_name=model_nn.name, key='q_loss', baseline=q_loss_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total loss\n",
    "fig, ax = plot_loss_hist(hist=hist, model_name=model_nn.name, key='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the training data\n",
    "# model_nn.evaluate(ds_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test data\n",
    "# model_nn.evaluate(ds_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed Sun-Earth-Jupiter System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sej_data import load_data_sej, make_datasets_sej, combine_datasets_sej\n",
    "import numpy as np\n",
    "from g3b_plot import plot_orbit_q, plot_orbit_v, plot_orbit_a, plot_orbit_energy, plot_orbit_element\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory length\n",
    "n_years = 100\n",
    "sample_freq = 10\n",
    "\n",
    "# Number of trajectories\n",
    "num_batches = 1\n",
    "n_traj = 100\n",
    "vt_split = 0.20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orbital perturbation scales\n",
    "sd_log_a = 0.01\n",
    "sd_log_e = 0.10\n",
    "sd_log_inc = 0.10\n",
    "sd_Omega = np.pi * 0.02\n",
    "sd_omega = np.pi * 0.02\n",
    "sd_f = np.pi * 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of seeds to use for datasets\n",
    "seed0 = 42\n",
    "seed1 = seed0 + num_batches * 3\n",
    "seeds = list(range(seed0, seed1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_sej(n_traj=n_traj, vt_split=vt_split, n_years=n_years, sample_freq=sample_freq,\n",
    "                     sd_log_a=sd_log_a, sd_log_e=sd_log_e, sd_log_inc=sd_log_inc,\n",
    "                     sd_Omega=sd_Omega, sd_omega=sd_omega, sd_f=sd_f, seed=seed0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_trn, outputs_trn = data[0:2]\n",
    "\n",
    "data_trn = {**inputs_trn, **outputs_trn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_trn['t'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trn['q'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs_trn['t'][0], outputs_trn['q'][0][:, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs_trn['t'][0], outputs_trn['q'][0][:, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outputs_trn['orb_a'], axis=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trn['orb_a'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outputs_trn['orb_a'][:, 0, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(outputs_trn['orb_a'][:, 0, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outputs_trn['orb_e'][:, 0, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(outputs_trn['orb_e'][:, 0, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbody]",
   "language": "python",
   "name": "conda-env-nbody-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
