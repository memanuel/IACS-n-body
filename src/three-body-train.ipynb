{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Body Problem: Neural Network Training\n",
    "\n",
    "A simple neural network is trained to learn approximate solutions to the three body problem.<br>\n",
    "Data is sampled from three body systems with parameters similar to planets in the solar system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import tensorflow as tf\n",
    "import rebound\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aliases\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from utils import load_vartbl, save_vartbl, plot_style\n",
    "from tf_utils import gpu_grow_memory, TimeHistory\n",
    "from tf_utils import plot_loss_hist, EpochLoss, TimeHistory\n",
    "from tf_utils import Identity\n",
    "\n",
    "from orbital_element import OrbitalElementToConfig, ConfigToOrbitalElement, MeanToTrueAnomaly, G_\n",
    "from orbital_element import make_model_elt_to_cfg, make_model_cfg_to_elt\n",
    "\n",
    "from jacobi import CartesianToJacobi, JacobiToCartesian\n",
    "\n",
    "from g3b_data import make_traj_g3b, make_data_g3b, make_datasets_g3b, traj_to_batch\n",
    "from g3b_data import make_datasets_solar, make_datasets_hard\n",
    "from g3b_data import combine_datasets_g3b, combine_datasets_solar\n",
    "from g3b_plot import plot_orbit_q, plot_orbit_v, plot_orbit_a, plot_orbit_energy, plot_orbit_element\n",
    "from g3b import KineticEnergy_G3B, PotentialEnergy_G3B, Momentum_G3B, AngularMomentum_G3B\n",
    "from g3b import VectorError, EnergyError\n",
    "from g3b import Motion_G3B, make_physics_model_g3b\n",
    "from g3b import fit_model\n",
    "from g3b_model_math import make_position_model_g3b_math, make_model_g3b_math\n",
    "from g3b_model_nn import make_position_model_g3b_nn, make_model_g3b_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set active GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[1:2], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grow GPU memory (must be first operation in TF)\n",
    "# gpu_grow_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight serialization\n",
    "fname = '../data/g3b/g3b_train.pickle'\n",
    "vartbl = load_vartbl(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of datasets to be loaded\n",
    "n_years = 100\n",
    "sample_freq = 10\n",
    "traj_size = n_years * sample_freq + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading data sets\n",
    "# num_data_sets = 50\n",
    "num_data_sets = 5\n",
    "batch_size = 64\n",
    "num_gpus = 1\n",
    "full_batch_size = num_gpus * batch_size\n",
    "seed0 = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/g3b/398004947.pickle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 11:01:58.203566 140426489472832 deprecation.py:323] From /home/michael/anaconda3/envs/nbody/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c688d8c95c4e8e9534f692772387ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/g3b/1492681748.pickle.\n",
      "Loaded data from ../data/g3b/3981900377.pickle.\n",
      "Loaded data from ../data/g3b/1029057319.pickle.\n",
      "Loaded data from ../data/g3b/1075961698.pickle.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build combined solar data sets\n",
    "ds_trn, ds_val, ds_tst = combine_datasets_solar(num_data_sets=num_data_sets, batch_size=batch_size, seed0=seed0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Math Model as a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_math = make_model_g3b_math(traj_size=traj_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0)\n",
    "\n",
    "loss = {'q': VectorError(name='q_loss'),\n",
    "        'v': VectorError(name='v_loss'),\n",
    "        'a': VectorError(regularizer=1.0, name='a_loss'),\n",
    "        'q0_rec': VectorError(name='q0_loss'),\n",
    "        'v0_rec': VectorError(name='v0_loss'),\n",
    "        'H': EnergyError(name='H_loss'),\n",
    "        'P': VectorError(name='P_loss', regularizer=1.0),\n",
    "        'L': VectorError(name='L_loss'),\n",
    "       }\n",
    "\n",
    "metrics = None\n",
    "\n",
    "loss_weights = {'q': 1.0,\n",
    "                'v': 1.0,\n",
    "                'a': 1.0,\n",
    "                'q0_rec': 1.0,\n",
    "                'v0_rec': 1.0,\n",
    "                'H': 1.0,\n",
    "                'P': 1.0,\n",
    "                'L': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the full mathematical model\n",
    "model_math.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for neural network model architecture\n",
    "hidden_sizes = [64, 16]\n",
    "skip_layers = True\n",
    "traj_size = 1001\n",
    "batch_size = 64\n",
    "\n",
    "# Training configuration\n",
    "activity_reg = 1.0E0\n",
    "learning_rate = 1.0E-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network model\n",
    "model_nn = make_model_g3b_nn(hidden_sizes=hidden_sizes, skip_layers=skip_layers, \n",
    "                             traj_size=traj_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss = {'q': VectorError(name='q_loss'),\n",
    "        'v': VectorError(name='v_loss'),\n",
    "        'a': VectorError(regularizer=1.0E-2, name='a_loss'),\n",
    "        'q0_rec': VectorError(name='q0_loss'),\n",
    "        'v0_rec': VectorError(name='v0_loss'),\n",
    "        'H': EnergyError(name='H_loss'),\n",
    "        'P': VectorError(name='P_loss', regularizer=1.0E-4),\n",
    "        'L': VectorError(name='L_loss'),\n",
    "       }\n",
    "\n",
    "metrics = None\n",
    "\n",
    "loss_weights = {'q': 1.0,\n",
    "                'v': 1.0,\n",
    "                'a': 1.0,\n",
    "                'q0_rec': 1.0E4,\n",
    "                'v0_rec': 1.0E4,\n",
    "                'H': 1.0,\n",
    "                'P': 1.0,\n",
    "                'L': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the NN model\n",
    "model_nn.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/155 [==============================] - 11s 68ms/step - loss: 0.0151 - q_loss: 0.0049 - v_loss: 0.0053 - a_loss: 0.0049 - q0_rec_loss: 3.2634e-14 - v0_rec_loss: 3.2553e-14 - H_loss: 2.4143e-08 - P_loss: 1.1818e-16 - L_loss: 2.0982e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015091853274832595,\n",
       " 0.0048598005,\n",
       " 0.0053437683,\n",
       " 0.004888256,\n",
       " 3.263435e-14,\n",
       " 3.2553102e-14,\n",
       " 2.414263e-08,\n",
       " 1.1817788e-16,\n",
       " 2.098198e-14]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the NN model on the validation data\n",
    "model_nn.evaluate(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this to math model - should be the same before training\n",
    "# loss = 0.0126\n",
    "# q_loss = 0.0049\n",
    "# model_math.evaluate(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "suffix = '_'.join(str(sz) for sz in hidden_sizes)\n",
    "model_name = f'model_g3b_nn_{suffix}'\n",
    "model_h5 = f'../models/g3b/{model_name}.h5'\n",
    "hist_name = model_name.replace('model_', 'hist_')\n",
    "epochs = 1\n",
    "save_freq = 'epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load model_g3b_nn_64_16 from ../models/g3b/model_g3b_nn_64_16.h5. Fitting...\n",
      "779/780 [============================>.] - ETA: 0s - loss: 2.0641 - q_loss: 0.0101 - v_loss: 0.0080 - a_loss: 0.0081 - q0_rec_loss: 9.0926e-05 - v0_rec_loss: 1.1275e-04 - H_loss: 7.8358e-04 - P_loss: 1.1741e-16 - L_loss: 2.2102e-04\n",
      "Epoch 0001; loss 2.06e+00; elapsed 0:01:14\n",
      "780/780 [==============================] - 65s 83ms/step - loss: 2.0616 - q_loss: 0.0101 - v_loss: 0.0080 - a_loss: 0.0081 - q0_rec_loss: 9.0813e-05 - v0_rec_loss: 1.1261e-04 - H_loss: 7.8258e-04 - P_loss: 1.1735e-16 - L_loss: 2.2074e-04\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_nn.load_weights(model_h5)\n",
    "    model_nn.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    hist = vartbl[hist_name]\n",
    "    print(f'Loaded {model_name} from {model_h5}.')\n",
    "except:\n",
    "    print(f'Unable to load {model_name} from {model_h5}. Fitting...')\n",
    "    hist = fit_model(model=model_nn, \n",
    "                     ds=ds_trn, \n",
    "                     epochs=epochs,\n",
    "                     loss=loss, \n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics,\n",
    "                     save_freq=save_freq,\n",
    "                     prev_history = None, \n",
    "                     batch_num=0)\n",
    "    vartbl[hist_name] = hist\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 50\n",
    "num_epochs = 1\n",
    "for i in range(num_epochs):\n",
    "    ts = datetime.datetime.now()\n",
    "    st = ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f'*** Training loop {i+1:3} *** - {st}')\n",
    "    hist = fit_model(model=model_nn, \n",
    "                     ds=ds_trn, \n",
    "                     epochs=epochs,\n",
    "                     loss=loss, \n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics,\n",
    "                     save_freq=save_freq,\n",
    "                     prev_history = hist, \n",
    "                     batch_num=i)\n",
    "    vartbl[hist_name] = hist\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "fig, ax = plot_loss_hist(hist, '[64, 16]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the training data\n",
    "model_nn.evaluate(ds_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test data\n",
    "model_nn.evaluate(ds_tst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbody]",
   "language": "python",
   "name": "conda-env-nbody-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
