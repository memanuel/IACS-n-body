{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Body Problem: Neural Network Training\n",
    "\n",
    "A simple neural network is trained to learn approximate solutions to the three body problem.<br>\n",
    "Data is sampled from three body systems with parameters similar to planets in the solar system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import tensorflow as tf\n",
    "import rebound\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aliases\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from utils import load_vartbl, save_vartbl, plot_style, range_inc\n",
    "from tf_utils import gpu_grow_memory, TimeHistory\n",
    "from tf_utils import plot_loss_hist, EpochLoss, TimeHistory\n",
    "from tf_utils import Identity\n",
    "\n",
    "from orbital_element import OrbitalElementToConfig, ConfigToOrbitalElement, MeanToTrueAnomaly, G_\n",
    "from orbital_element import make_model_elt_to_cfg, make_model_cfg_to_elt\n",
    "\n",
    "from jacobi import CartesianToJacobi, JacobiToCartesian\n",
    "\n",
    "from g3b_data import make_traj_g3b, make_data_g3b, make_datasets_g3b, traj_to_batch\n",
    "from g3b_data import make_datasets_solar, make_datasets_hard\n",
    "from g3b_data import combine_datasets_g3b, combine_datasets_solar\n",
    "from sej_data import load_data_sej, make_datasets_sej, combine_datasets_sej\n",
    "\n",
    "from g3b_plot import plot_orbit_q, plot_orbit_v, plot_orbit_a, plot_orbit_energy, plot_orbit_element\n",
    "from g3b import KineticEnergy_G3B, PotentialEnergy_G3B, Momentum_G3B, AngularMomentum_G3B\n",
    "from g3b import VectorError, EnergyError\n",
    "from g3b import Motion_G3B, make_physics_model_g3b\n",
    "from g3b import fit_model\n",
    "from g3b_model_math import make_position_model_g3b_math, make_model_g3b_math\n",
    "from g3b_model_nn import make_position_model_g3b_nn, make_model_g3b_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set active GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[1:2], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grow GPU memory (must be first operation in TF)\n",
    "# gpu_grow_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight serialization\n",
    "fname = '../data/g3b/g3b_train.pickle'\n",
    "vartbl = load_vartbl(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data for General Three Body Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of datasets to be loaded\n",
    "n_years = 100\n",
    "sample_freq = 10\n",
    "traj_size = n_years * sample_freq + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading data sets\n",
    "# num_data_sets = 50\n",
    "num_data_sets = 5\n",
    "batch_size = 256\n",
    "# num_gpus = 1\n",
    "# full_batch_size = num_gpus * batch_size\n",
    "\n",
    "# Set size of tiny data sets\n",
    "n_traj_tiny = batch_size\n",
    "\n",
    "# Set starting random seed\n",
    "seed0 = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/g3b/1789961721.pickle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 11:16:47.555900 139880889796416 deprecation.py:323] From /home/michael/anaconda3/envs/nbody/lib/python3.7/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Create a tiny data set with one batch of solar type orbits\n",
    "ds_tiny_solar, _ , _ = \\\n",
    "    make_datasets_solar(n_traj=n_traj_tiny, vt_split=0.0, \n",
    "                        n_years=n_years, sample_freq=sample_freq,\n",
    "                        batch_size=batch_size, seed=seed0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined solar data sets\n",
    "# ds_solar_trn, ds_solar_val, ds_solar_tst = \\\n",
    "#     combine_datasets_solar(num_data_sets=num_data_sets, batch_size=batch_size, seed0=seed0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data for Perturbed Sun-Earth-Jupiter System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orbital perturbation scales\n",
    "sd_log_a = 0.01\n",
    "sd_log_e = 0.10\n",
    "sd_log_inc = 0.10\n",
    "sd_Omega = np.pi * 0.02\n",
    "sd_omega = np.pi * 0.02\n",
    "sd_f = np.pi * 0.02\n",
    "\n",
    "# Wrap into dictionary\n",
    "sej_sigma = {\n",
    "    'sd_log_a': sd_log_a,\n",
    "    'sd_log_e': sd_log_e,\n",
    "    'sd_log_inc': sd_log_inc,\n",
    "    'sd_Omega': sd_Omega,\n",
    "    'sd_omega': sd_omega,\n",
    "    'sd_f': sd_f\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/sej/1026452775.pickle.\n"
     ]
    }
   ],
   "source": [
    "# Create a tiny data set with one batch of perturbed SEJ orbits\n",
    "ds_tiny_sej, _ , _ = \\\n",
    "    make_datasets_sej(n_traj=n_traj_tiny, vt_split=0.0, n_years=n_years, sample_freq=sample_freq,\n",
    "                      **sej_sigma,\n",
    "                      batch_size=batch_size, seed=seed0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/sej/3203691191.pickle.\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary for sigmas of unperturbed orbits: all sd are zero (always same elements)\n",
    "sej_sigma0 = {k: v*0.0 for k, v in sej_sigma.items()}\n",
    "\n",
    "# Create a tiny data set with the unperturbed SEJ system\n",
    "ds_sej0, _, _ = \\\n",
    "    make_datasets_sej(n_traj=n_traj_tiny, vt_split=0.0, n_years=n_years, sample_freq=sample_freq,\n",
    "                      **sej_sigma0,\n",
    "                      batch_size=batch_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/sej/4087833051.pickle.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7d5e61da3e4464af0eb75e48d7e118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/sej/3169253148.pickle.\n",
      "Loaded data from ../data/sej/155748689.pickle.\n",
      "Loaded data from ../data/sej/3645397039.pickle.\n",
      "Loaded data from ../data/sej/2755636330.pickle.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build combined SEJ data sets\n",
    "ds_sej_trn, ds_sej_val, ds_sej_tst = \\\n",
    "    combine_datasets_sej(num_data_sets=num_data_sets, batch_size=batch_size, seed0=seed0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Choose Data Set for Analysis: Solar vs. SEJ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alias ds_trn, ds_val, ds_tst to selected source\n",
    "\n",
    "# The selected data type for this analysis\n",
    "data_type = 'SEJ'\n",
    "\n",
    "# Tables mapping data type to tuple of data sets\n",
    "data_by_type = {\n",
    "    # 'solar': (ds_tiny_solar, ds_solar_trn, ds_solar_val, ds_solar_tst),\n",
    "    'SEJ': (ds_tiny_sej, ds_sej_trn, ds_sej_val, ds_sej_tst)\n",
    "}\n",
    "\n",
    "# Perform the aliasing\n",
    "ds_tiny, ds_trn, ds_val, ds_tst = data_by_type[data_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Kepler-Jacobi Model as a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kj = make_model_g3b_math(traj_size=traj_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0)\n",
    "\n",
    "loss = {'q': VectorError(name='q_loss'),\n",
    "        'v': VectorError(name='v_loss'),\n",
    "        'a': VectorError(regularizer=1.0, name='a_loss'),\n",
    "        'q0_rec': VectorError(name='q0_loss'),\n",
    "        'v0_rec': VectorError(name='v0_loss'),\n",
    "        'H': EnergyError(name='H_loss'),\n",
    "        'P': VectorError(name='P_loss', regularizer=1.0E-6),\n",
    "        'L': VectorError(name='L_loss'),\n",
    "       }\n",
    "\n",
    "metrics = None\n",
    "\n",
    "loss_weights = {'q': 1.0,\n",
    "                'v': 1.0,\n",
    "                'a': 1.0,\n",
    "                'q0_rec': 1.0E4,\n",
    "                'v0_rec': 1.0E4,\n",
    "                'H': 1.0,\n",
    "                'P': 1.0,\n",
    "                'L': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the full mathematical model\n",
    "model_kj.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 2s 2s/step - loss: 6.6520e-05 - q_loss: 1.9148e-05 - v_loss: 1.9136e-05 - a_loss: 2.8235e-05 - q0_rec_loss: 4.1165e-14 - v0_rec_loss: 9.4240e-15 - H_loss: 2.2975e-13 - P_loss: 4.1866e-14 - L_loss: 7.2123e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.651984585914761e-05,\n",
       " 1.9148267e-05,\n",
       " 1.9135909e-05,\n",
       " 2.8235167e-05,\n",
       " 4.116526e-14,\n",
       " 9.423976e-15,\n",
       " 2.2974615e-13,\n",
       " 4.1866443e-14,\n",
       " 7.2123155e-14]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate KJ model on unperturbed SEJ data set\n",
    "model_kj.evaluate(ds_sej0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.4279e-05 - q_loss: 1.8427e-05 - v_loss: 1.8415e-05 - a_loss: 2.7436e-05 - q0_rec_loss: 1.4879e-14 - v0_rec_loss: 1.3018e-14 - H_loss: 9.3354e-14 - P_loss: 3.8772e-14 - L_loss: 2.0240e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.427881453419104e-05,\n",
       " 1.8427074e-05,\n",
       " 1.8414992e-05,\n",
       " 2.7436467e-05,\n",
       " 1.4878642e-14,\n",
       " 1.3017593e-14,\n",
       " 9.33544e-14,\n",
       " 3.877194e-14,\n",
       " 2.0240327e-14]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate KJ model on tiny data set\n",
    "model_kj.evaluate(ds_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 4s 107ms/step - loss: 6.5870e-05 - q_loss: 1.8911e-05 - v_loss: 1.8899e-05 - a_loss: 2.8060e-05 - q0_rec_loss: 1.4463e-14 - v0_rec_loss: 1.2528e-14 - H_loss: 9.3760e-14 - P_loss: 3.8766e-14 - L_loss: 1.9768e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.587040261365473e-05,\n",
       " 1.8911158e-05,\n",
       " 1.8898823e-05,\n",
       " 2.806015e-05,\n",
       " 1.4462772e-14,\n",
       " 1.2528137e-14,\n",
       " 9.376004e-14,\n",
       " 3.876589e-14,\n",
       " 1.9768472e-14]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate KJ model on full validation data\n",
    "model_kj.evaluate(ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for neural network model architecture\n",
    "# hidden_sizes = [64, 16]\n",
    "hidden_sizes = []\n",
    "skip_layers = True\n",
    "traj_size = 1001\n",
    "\n",
    "# Training configuration\n",
    "reg = 1.0E2\n",
    "kernel_reg = reg\n",
    "activity_reg = reg\n",
    "learning_rate = 1.0E-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network model\n",
    "model_nn = make_model_g3b_nn(hidden_sizes=hidden_sizes, skip_layers=skip_layers, \n",
    "                             kernel_reg=kernel_reg, activity_reg=activity_reg,\n",
    "                             traj_size=traj_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# optimizer = keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "# optimizer = keras.optimizers.Adadelta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the NN model\n",
    "model_nn.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 2s 2s/step - loss: 6.4279e-05 - q_loss: 1.8427e-05 - v_loss: 1.8415e-05 - a_loss: 2.7436e-05 - q0_rec_loss: 1.4879e-14 - v0_rec_loss: 1.3018e-14 - H_loss: 9.3354e-14 - P_loss: 3.8772e-14 - L_loss: 2.0240e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.427881453419104e-05,\n",
       " 1.8427074e-05,\n",
       " 1.8414992e-05,\n",
       " 2.7436467e-05,\n",
       " 1.4878642e-14,\n",
       " 1.3017593e-14,\n",
       " 9.33544e-14,\n",
       " 3.877194e-14,\n",
       " 2.0240327e-14]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the NN model on the tiny data set\n",
    "model_nn.evaluate(ds_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 4s 113ms/step - loss: 6.5870e-05 - q_loss: 1.8911e-05 - v_loss: 1.8899e-05 - a_loss: 2.8060e-05 - q0_rec_loss: 1.4463e-14 - v0_rec_loss: 1.2528e-14 - H_loss: 9.3760e-14 - P_loss: 3.8766e-14 - L_loss: 1.9768e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.587040261365473e-05,\n",
       " 1.8911158e-05,\n",
       " 1.8898823e-05,\n",
       " 2.806015e-05,\n",
       " 1.4462772e-14,\n",
       " 1.2528137e-14,\n",
       " 9.376004e-14,\n",
       " 3.876589e-14,\n",
       " 1.9768472e-14]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the NN model on the full validation data\n",
    "model_nn.evaluate(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this to math model - should be the same before training\n",
    "# model_math.evaluate(ds_val)\n",
    "\n",
    "# Hard coded baseline losses\n",
    "loss_baseline_list = \\\n",
    "[6.427881453419104e-05,\n",
    " 1.8427074e-05,\n",
    " 1.8414992e-05,\n",
    " 2.7436467e-05,\n",
    " 1.4878642e-14,\n",
    " 1.3017593e-14,\n",
    " 9.33544e-14,\n",
    " 3.877194e-14,\n",
    " 2.0240327e-14]\n",
    "\n",
    "# Baseline position loss\n",
    "q_loss_baseline = loss_baseline_list[1]\n",
    "\n",
    "# Table of baseline losses\n",
    "keys = ['loss', 'q_loss', 'v_loss', 'a_loss', 'q0_rec_loss', 'v0_rec_loss', 'H_loss', 'P_loss', 'L_loss']\n",
    "loss_baseline = {key: loss_baseline_list[i] for i, key in enumerate(keys)}\n",
    "# Set dummy batch_num and time\n",
    "loss_baseline['batch_num'] = 0\n",
    "loss_baseline['time'] = 0.0\n",
    "\n",
    "# Initialize history before training\n",
    "hist0 = {key: np.array([val], dtype=np.float32) for key, val in loss_baseline.items()}\n",
    "\n",
    "# Review baseline loss table\n",
    "# loss_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "suffix = '_'.join(str(sz) for sz in hidden_sizes)\n",
    "if data_type == 'solar':\n",
    "    model_name = f'model_g3b_nn_{suffix}'\n",
    "    folder = 'g3b'\n",
    "elif data_type == 'SEJ':\n",
    "    model_name = f'model_sej_nn_{suffix}'\n",
    "    folder = 'sej'\n",
    "model_h5 = f'../models/g3b/{model_name}.h5'\n",
    "hist_name = model_name.replace('model_', 'hist_')\n",
    "epochs = 1\n",
    "save_freq = 'epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0001; loss 6.43e-05; elapsed 0:00:06\n",
      "\r",
      "1/1 [==============================] - 6s 6s/step - loss: 6.4279e-05 - q_loss: 1.8427e-05 - v_loss: 1.8415e-05 - a_loss: 2.7436e-05 - q0_rec_loss: 1.4879e-14 - v0_rec_loss: 1.3018e-14 - H_loss: 9.3354e-14 - P_loss: 3.8772e-14 - L_loss: 2.0240e-14\n"
     ]
    }
   ],
   "source": [
    "hist = fit_model(model=model_nn,\n",
    "                 folder=folder,\n",
    "                 ds=ds_tiny, \n",
    "                 epochs=epochs,\n",
    "                 save_freq=save_freq,\n",
    "                 prev_history = hist0, \n",
    "                 batch_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to load model_sej_nn_ from ../models/g3b/model_sej_nn_.h5. Fitting...\n",
      "\n",
      "Epoch 0001; loss 6.52e+03; elapsed 0:00:00\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 6524.2783 - q_loss: 3.9260e-04 - v_loss: 3.7419e-04 - a_loss: 2.5434e-04 - q0_rec_loss: 6.8264e-05 - v0_rec_loss: 7.1417e-05 - H_loss: 9.3492e-07 - P_loss: 3.9335e-14 - L_loss: 3.9487e-07\n"
     ]
    }
   ],
   "source": [
    "# Attempt to load model or train a single epoch\n",
    "try:\n",
    "    model_nn.load_weights(model_h5)\n",
    "    model_nn.compile(loss=loss, optimizer=optimizer, metrics=metrics, loss_weights=loss_weights)\n",
    "    hist = vartbl[hist_name]\n",
    "    print(f'Loaded {model_name} from {model_h5}.')\n",
    "except:\n",
    "    print(f'Unable to load {model_name} from {model_h5}. Fitting...')\n",
    "    hist = fit_model(model=model_nn,\n",
    "                     folder=folder,\n",
    "                     # ds=ds_trn, \n",
    "                     ds=ds_tiny, \n",
    "                     epochs=epochs,\n",
    "                     save_freq=save_freq,\n",
    "                     prev_history = hist0, \n",
    "                     batch_num=1)\n",
    "    vartbl[hist_name] = hist\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 50\n",
    "num_epochs = 1\n",
    "for i in range_inc(1, num_epochs):\n",
    "    ts = datetime.datetime.now()\n",
    "    st = ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f'*** Training loop {i:3} *** - {st}')\n",
    "    hist = fit_model(model=model_nn,\n",
    "                     folder=folder,\n",
    "                     ds=ds_trn, \n",
    "                     epochs=epochs,\n",
    "                     loss=loss, \n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics,\n",
    "                     save_freq=save_freq,\n",
    "                     prev_history = hist, \n",
    "                     batch_num=i)\n",
    "    vartbl[hist_name] = hist\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "fig, ax = plot_loss_hist(hist=hist, model_name=model_nn.name, key='q_loss', baseline=q_loss_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total loss\n",
    "fig, ax = plot_loss_hist(hist=hist, model_name=model_nn.name, key='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the training data\n",
    "# model_nn.evaluate(ds_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test data\n",
    "# model_nn.evaluate(ds_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed Sun-Earth-Jupiter System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sej_data import load_data_sej, make_datasets_sej, combine_datasets_sej\n",
    "import numpy as np\n",
    "from g3b_plot import plot_orbit_q, plot_orbit_v, plot_orbit_a, plot_orbit_energy, plot_orbit_element\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory length\n",
    "n_years = 100\n",
    "sample_freq = 10\n",
    "\n",
    "# Number of trajectories\n",
    "num_batches = 1\n",
    "n_traj = 100\n",
    "vt_split = 0.20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orbital perturbation scales\n",
    "sd_log_a = 0.01\n",
    "sd_log_e = 0.10\n",
    "sd_log_inc = 0.10\n",
    "sd_Omega = np.pi * 0.02\n",
    "sd_omega = np.pi * 0.02\n",
    "sd_f = np.pi * 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of seeds to use for datasets\n",
    "seed0 = 42\n",
    "seed1 = seed0 + num_batches * 3\n",
    "seeds = list(range(seed0, seed1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data_sej(n_traj=n_traj, vt_split=vt_split, n_years=n_years, sample_freq=sample_freq,\n",
    "                     sd_log_a=sd_log_a, sd_log_e=sd_log_e, sd_log_inc=sd_log_inc,\n",
    "                     sd_Omega=sd_Omega, sd_omega=sd_omega, sd_f=sd_f, seed=seed0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_trn, outputs_trn = data[0:2]\n",
    "\n",
    "data_trn = {**inputs_trn, **outputs_trn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_trn['t'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trn['q'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs_trn['t'][0], outputs_trn['q'][0][:, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs_trn['t'][0], outputs_trn['q'][0][:, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outputs_trn['orb_a'], axis=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_trn['orb_a'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outputs_trn['orb_a'][:, 0, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(outputs_trn['orb_a'][:, 0, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outputs_trn['orb_e'][:, 0, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(outputs_trn['orb_e'][:, 0, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbody]",
   "language": "python",
   "name": "conda-env-nbody-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
