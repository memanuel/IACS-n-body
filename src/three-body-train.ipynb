{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Body Problem: Neural Network Training\n",
    "\n",
    "A simple neural network is trained to learn approximate solutions to the three body problem.<br>\n",
    "Data is sampled from three body systems with parameters similar to planets in the solar system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import tensorflow as tf\n",
    "import rebound\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aliases\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from utils import load_vartbl, save_vartbl, plot_style, range_inc\n",
    "from tf_utils import gpu_grow_memory, TimeHistory\n",
    "from tf_utils import plot_loss_hist, EpochLoss, TimeHistory\n",
    "from tf_utils import Identity\n",
    "\n",
    "from orbital_element import OrbitalElementToConfig, ConfigToOrbitalElement, MeanToTrueAnomaly, G_\n",
    "from orbital_element import make_model_elt_to_cfg, make_model_cfg_to_elt\n",
    "\n",
    "from jacobi import CartesianToJacobi, JacobiToCartesian\n",
    "\n",
    "from g3b_data import make_traj_g3b, make_data_g3b, make_datasets_g3b, traj_to_batch\n",
    "from g3b_data import make_datasets_solar, make_datasets_hard\n",
    "from g3b_data import combine_datasets_g3b, combine_datasets_solar\n",
    "from g3b_plot import plot_orbit_q, plot_orbit_v, plot_orbit_a, plot_orbit_energy, plot_orbit_element\n",
    "from g3b import KineticEnergy_G3B, PotentialEnergy_G3B, Momentum_G3B, AngularMomentum_G3B\n",
    "from g3b import VectorError, EnergyError\n",
    "from g3b import Motion_G3B, make_physics_model_g3b\n",
    "from g3b import fit_model\n",
    "from g3b_model_math import make_position_model_g3b_math, make_model_g3b_math\n",
    "from g3b_model_nn import make_position_model_g3b_nn, make_model_g3b_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set active GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[1:2], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grow GPU memory (must be first operation in TF)\n",
    "# gpu_grow_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight serialization\n",
    "fname = '../data/g3b/g3b_train.pickle'\n",
    "vartbl = load_vartbl(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of datasets to be loaded\n",
    "n_years = 100\n",
    "sample_freq = 10\n",
    "traj_size = n_years * sample_freq + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading data sets\n",
    "# num_data_sets = 50\n",
    "num_data_sets = 20\n",
    "batch_size = 256\n",
    "# num_gpus = 1\n",
    "# full_batch_size = num_gpus * batch_size\n",
    "seed0 = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny data set with 64 solar type orbits\n",
    "n_traj_tiny = 64\n",
    "ds_tiny_trn, ds_tiny_val, ds_tiny_tst = make_datasets_solar(n_traj=n_traj_tiny, vt_split=1.0, \n",
    "                                                            n_years=n_years, sample_freq=sample_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined solar data sets\n",
    "ds_trn, ds_val, ds_tst = combine_datasets_solar(num_data_sets=num_data_sets, batch_size=batch_size, seed0=seed0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Math Model as a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_math = make_model_g3b_math(traj_size=traj_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0)\n",
    "\n",
    "loss = {'q': VectorError(name='q_loss'),\n",
    "        'v': VectorError(name='v_loss'),\n",
    "        'a': VectorError(regularizer=1.0, name='a_loss'),\n",
    "        'q0_rec': VectorError(name='q0_loss'),\n",
    "        'v0_rec': VectorError(name='v0_loss'),\n",
    "        'H': EnergyError(name='H_loss'),\n",
    "        'P': VectorError(name='P_loss', regularizer=1.0),\n",
    "        'L': VectorError(name='L_loss'),\n",
    "       }\n",
    "\n",
    "metrics = None\n",
    "\n",
    "loss_weights = {'q': 1.0,\n",
    "                'v': 1.0,\n",
    "                'a': 1.0,\n",
    "                'q0_rec': 1.0E4,\n",
    "                'v0_rec': 1.0E4,\n",
    "                'H': 1.0,\n",
    "                'P': 1.0,\n",
    "                'L': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the full mathematical model\n",
    "model_math.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for neural network model architecture\n",
    "# hidden_sizes = [64, 16]\n",
    "hidden_sizes = []\n",
    "skip_layers = True\n",
    "traj_size = 1001\n",
    "\n",
    "# Training configuration\n",
    "activity_reg = 1.0E-2\n",
    "learning_rate = 1.0E-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network model\n",
    "model_nn = make_model_g3b_nn(hidden_sizes=hidden_sizes, skip_layers=skip_layers, \n",
    "                             traj_size=traj_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# optimizer = keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "optimizer = keras.optimizers.Adadelta()\n",
    "\n",
    "loss = {'q': VectorError(name='q_loss'),\n",
    "        'v': VectorError(name='v_loss'),\n",
    "        'a': VectorError(regularizer=1.0E-2, name='a_loss'),\n",
    "        'q0_rec': VectorError(name='q0_loss'),\n",
    "        'v0_rec': VectorError(name='v0_loss'),\n",
    "        'H': EnergyError(name='H_loss'),\n",
    "        'P': VectorError(name='P_loss', regularizer=1.0E-4),\n",
    "        'L': VectorError(name='L_loss'),\n",
    "       }\n",
    "\n",
    "metrics = None\n",
    "\n",
    "loss_weights = {'q': 1.0,\n",
    "                'v': 1.0,\n",
    "                'a': 1.0,\n",
    "                'q0_rec': 1.0E4,\n",
    "                'v0_rec': 1.0E4,\n",
    "                'H': 1.0,\n",
    "                'P': 1.0,\n",
    "                'L': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the NN model\n",
    "model_nn.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the NN model on the validation data\n",
    "# model_nn.evaluate(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this to math model - should be the same before training\n",
    "# model_math.evaluate(ds_val)\n",
    "\n",
    "# Hard coded baseline losses\n",
    "loss_baseline_list = \\\n",
    "[0.015091853152567168,\n",
    " 0.0048598005,\n",
    " 0.0053437687,\n",
    " 0.004888256,\n",
    " 3.263435e-14,\n",
    " 3.2603365e-14,\n",
    " 2.4142583e-08,\n",
    " 1.1821102e-16,\n",
    " 2.1032521e-14]\n",
    "\n",
    "# Baseline position loss\n",
    "q_loss_baseline = loss_baseline_list[1]\n",
    "\n",
    "# Table of baseline losses\n",
    "keys = ['loss', 'q_loss', 'v_loss', 'a_loss', 'q0_rec_loss', 'v0_rec_loss', 'H_loss', 'P_loss', 'L_loss']\n",
    "loss_baseline = {key: loss_baseline_list[i] for i, key in enumerate(keys)}\n",
    "# Set dummy batch_num and time\n",
    "loss_baseline['batch_num'] = 0\n",
    "loss_baseline['time'] = 0.0\n",
    "\n",
    "# Initialize history before training\n",
    "hist0 = {key: np.array([val], dtype=np.float32) for key, val in loss_baseline.items()}\n",
    "\n",
    "# Review baseline loss table\n",
    "# loss_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "suffix = '_'.join(str(sz) for sz in hidden_sizes)\n",
    "model_name = f'model_g3b_nn_{suffix}'\n",
    "model_h5 = f'../models/g3b/{model_name}.h5'\n",
    "hist_name = model_name.replace('model_', 'hist_')\n",
    "epochs = 1\n",
    "save_freq = 'epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to load model or train a single epoch\n",
    "try:\n",
    "    model_nn.load_weights(model_h5)\n",
    "    model_nn.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    hist = vartbl[hist_name]\n",
    "    print(f'Loaded {model_name} from {model_h5}.')\n",
    "except:\n",
    "    print(f'Unable to load {model_name} from {model_h5}. Fitting...')\n",
    "    hist = fit_model(model=model_nn, \n",
    "                     ds=ds_trn, \n",
    "                     epochs=epochs,\n",
    "                     loss=loss, \n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics,\n",
    "                     save_freq=save_freq,\n",
    "                     prev_history = hist0, \n",
    "                     batch_num=1)\n",
    "    vartbl[hist_name] = hist\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 50\n",
    "num_epochs = 0\n",
    "for i in range_inc(1, num_epochs):\n",
    "    ts = datetime.datetime.now()\n",
    "    st = ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f'*** Training loop {i:3} *** - {st}')\n",
    "    hist = fit_model(model=model_nn, \n",
    "                     ds=ds_trn, \n",
    "                     epochs=epochs,\n",
    "                     loss=loss, \n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics,\n",
    "                     save_freq=save_freq,\n",
    "                     prev_history = hist, \n",
    "                     batch_num=i)\n",
    "    vartbl[hist_name] = hist\n",
    "    save_vartbl(vartbl, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "fig, ax = plot_loss_hist(hist=hist, model_name=model_nn.name, key='q_loss', baseline=q_loss_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total loss\n",
    "fig, ax = plot_loss_hist(hist=hist, model_name=model_nn.name, key='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the training data\n",
    "# model_nn.evaluate(ds_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test data\n",
    "# model_nn.evaluate(ds_tst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbody]",
   "language": "python",
   "name": "conda-env-nbody-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
