{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.0.0-dev20190606\n",
      "Eager mode:  True\n",
      "Hub version:  0.4.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\n",
    "\n",
    "(train_data, validation_data), test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=(train_validation_split, tfds.Split.TEST),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=231, shape=(), dtype=string, numpy=b\"As a lifelong fan of Dickens, I have invariably been disappointed by adaptations of his novels.<br /><br />Although his works presented an extremely accurate re-telling of human life at every level in Victorian Britain, throughout them all was a pervasive thread of humour that could be both playful or sarcastic as the narrative dictated. In a way, he was a literary caricaturist and cartoonist. He could be serious and hilarious in the same sentence. He pricked pride, lampooned arrogance, celebrated modesty, and empathised with loneliness and poverty. It may be a clich\\xc3\\xa9, but he was a people's writer.<br /><br />And it is the comedy that is so often missing from his interpretations. At the time of writing, Oliver Twist is being dramatised in serial form on BBC television. All of the misery and cruelty is their, but non of the humour, irony, and savage lampoonery. The result is just a dark, dismal experience: the story penned by a journalist rather than a novelist. It's not really Dickens at all.<br /><br />'Oliver!', on the other hand, is much closer to the mark. The mockery of officialdom is perfectly interpreted, from the blustering beadle to the drunken magistrate. The classic stand-off between the beadle and Mr Brownlow, in which the law is described as 'a ass, a idiot' couldn't have been better done. Harry Secombe is an ideal choice.<br /><br />But the blinding cruelty is also there, the callous indifference of the state, the cold, hunger, poverty and loneliness are all presented just as surely as The Master would have wished.<br /><br />And then there is crime. Ron Moody is a treasure as the sleazy Jewish fence, whilst Oliver Reid has Bill Sykes to perfection.<br /><br />Perhaps not surprisingly, Lionel Bart - himself a Jew from London's east-end - takes a liberty with Fagin by re-interpreting him as a much more benign fellow than was Dicken's original. In the novel, he was utterly ruthless, sending some of his own boys to the gallows in order to protect himself (though he was also caught and hanged). Whereas in the movie, he is presented as something of a wayward father-figure, a sort of charitable thief rather than a corrupter of children, the latter being a long-standing anti-semitic sentiment. Otherwise, very few liberties are taken with Dickens's original. All of the most memorable elements are included. Just enough menace and violence is retained to ensure narrative fidelity whilst at the same time allowing for children' sensibilities. Nancy is still beaten to death, Bullseye narrowly escapes drowning, and Bill Sykes gets a faithfully graphic come-uppance.<br /><br />Every song is excellent, though they do incline towards schmaltz. Mark Lester mimes his wonderfully. Both his and my favourite scene is the one in which the world comes alive to 'who will buy'. It's schmaltzy, but it's Dickens through and through.<br /><br />I could go on. I could commend the wonderful set-pieces, the contrast of the rich and poor. There is top-quality acting from more British regulars than you could shake a stick at.<br /><br />I ought to give it 10 points, but I'm feeling more like Scrooge today. Soak it up with your Christmas dinner. No original has been better realised.\">"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "train_examples_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=227, shape=(10,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 0, 1, 1, 0])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=414, shape=(3, 20), dtype=float32, numpy=\n",
       "array([[ 3.9819887 , -4.4838037 ,  5.177359  , -2.3643482 , -3.2938678 ,\n",
       "        -3.5364532 , -2.4786978 ,  2.5525482 ,  6.688532  , -2.3076782 ,\n",
       "        -1.9807833 ,  1.1315885 , -3.0339816 , -0.7604128 , -5.743445  ,\n",
       "         3.4242578 ,  4.790099  , -4.03061   , -5.992149  , -1.7297493 ],\n",
       "       [ 3.4232912 , -4.230874  ,  4.1488533 , -0.29553518, -6.802391  ,\n",
       "        -2.5163853 , -4.4002395 ,  1.905792  ,  4.7512794 , -0.40538004,\n",
       "        -4.3401685 ,  1.0361497 ,  0.9744097 ,  0.71507156, -6.2657013 ,\n",
       "         0.16533905,  4.560262  , -1.3106939 , -3.1121316 , -2.1338716 ],\n",
       "       [ 3.8508697 , -5.003031  ,  4.8700504 , -0.04324996, -5.893603  ,\n",
       "        -5.2983093 , -4.004676  ,  4.1236343 ,  6.267754  ,  0.11632943,\n",
       "        -3.5934832 ,  0.8023905 ,  0.56146765,  0.9192484 , -7.3066816 ,\n",
       "         2.8202746 ,  6.2000837 , -3.5709393 , -4.564525  , -2.305622  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples_batch[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 400,373\n",
      "Trainable params: 400,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0606 15:22:47.255475 139749122434880 deprecation.py:323] From /home/michael/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1251: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 7s 221ms/step - loss: 0.7035 - accuracy: 0.5578 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 5s 175ms/step - loss: 0.6162 - accuracy: 0.6775 - val_loss: 0.5943 - val_accuracy: 0.7019\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 5s 167ms/step - loss: 0.5708 - accuracy: 0.7216 - val_loss: 0.5581 - val_accuracy: 0.7244\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 5s 171ms/step - loss: 0.5333 - accuracy: 0.7459 - val_loss: 0.5257 - val_accuracy: 0.7509\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 5s 169ms/step - loss: 0.4967 - accuracy: 0.7733 - val_loss: 0.4937 - val_accuracy: 0.7725\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 5s 169ms/step - loss: 0.4608 - accuracy: 0.7950 - val_loss: 0.4636 - val_accuracy: 0.7888\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 5s 168ms/step - loss: 0.4270 - accuracy: 0.8137 - val_loss: 0.4371 - val_accuracy: 0.8045\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 5s 165ms/step - loss: 0.3959 - accuracy: 0.8319 - val_loss: 0.4136 - val_accuracy: 0.8165\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 5s 168ms/step - loss: 0.3673 - accuracy: 0.8455 - val_loss: 0.3929 - val_accuracy: 0.8271\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 5s 164ms/step - loss: 0.3406 - accuracy: 0.8584 - val_loss: 0.3747 - val_accuracy: 0.8360\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 5s 164ms/step - loss: 0.3161 - accuracy: 0.8717 - val_loss: 0.3594 - val_accuracy: 0.8436\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 5s 166ms/step - loss: 0.2939 - accuracy: 0.8841 - val_loss: 0.3465 - val_accuracy: 0.8484\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 5s 164ms/step - loss: 0.2738 - accuracy: 0.8933 - val_loss: 0.3358 - val_accuracy: 0.8542\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 5s 165ms/step - loss: 0.2555 - accuracy: 0.9018 - val_loss: 0.3270 - val_accuracy: 0.8589\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 5s 166ms/step - loss: 0.2386 - accuracy: 0.9102 - val_loss: 0.3198 - val_accuracy: 0.8623\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 5s 164ms/step - loss: 0.2231 - accuracy: 0.9169 - val_loss: 0.3141 - val_accuracy: 0.8653\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 5s 167ms/step - loss: 0.2088 - accuracy: 0.9233 - val_loss: 0.3097 - val_accuracy: 0.8662\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 5s 164ms/step - loss: 0.1954 - accuracy: 0.9294 - val_loss: 0.3064 - val_accuracy: 0.8678\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 5s 165ms/step - loss: 0.1831 - accuracy: 0.9353 - val_loss: 0.3041 - val_accuracy: 0.8685\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 5s 167ms/step - loss: 0.1715 - accuracy: 0.9405 - val_loss: 0.3027 - val_accuracy: 0.8701\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                   epochs=20,\n",
    "                   validation_data=validation_data.batch(512),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
